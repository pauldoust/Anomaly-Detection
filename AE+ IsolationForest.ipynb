{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXtjgU8PJAOk",
        "colab_type": "text"
      },
      "source": [
        "Mount Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_1TcmrI8N8",
        "colab_type": "code",
        "outputId": "86cc8518-58e0-4e18-971f-cea8c06817c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/My Drive/MLDM Project/data.zip\"\n",
        "\n",
        "!unzip '/content/drive/My Drive/MLDM Project/newDataset.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/My Drive/MLDM Project/data.zip\n",
            "  inflating: validation.hdf5         \n",
            "  inflating: train.hdf5              \n",
            "Archive:  /content/drive/My Drive/MLDM Project/newDataset.zip\n",
            "  inflating: data.h5                 \n",
            "  inflating: validation.h5           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cSs7R2FJFtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,CuDNNLSTM, Conv1D, MaxPooling1D,UpSampling1D, Input, Lambda, AveragePooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.utils import plot_model\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.covariance import EllipticEnvelope\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3tCqCw8JMP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_name = r\"/content/data.h5\"\n",
        "test_file_name = r\"/content/validation.h5\"\n",
        "labels = pd.read_csv(\"/content/drive/My Drive/MLDM Project/labels.csv\", sep=\",\")\n",
        "\n",
        "\n",
        "time_series = pd.read_hdf(train_file_name)\n",
        "test_time_series = pd.read_hdf(test_file_name)\n",
        "# time_series.iloc[0].index = time_series.iloc[0].index.astype(dtype='int64', copy=False)\n",
        "labels = labels[\"anomaly\"].to_numpy().astype(int)\n",
        "labels = labels.reshape(-1, 1)\n",
        "\n",
        "X_train = time_series\n",
        "X_test = test_time_series\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# scaler = RobustScaler()\n",
        "# scaler = StandardScaler()\n",
        "scaler.fit(X_train) \n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL4s49JoJMc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Based on work by Xianshun Chen\n",
        "https://github.com/chen0040/keras-anomaly-detection\n",
        "'''\n",
        "\n",
        "from keras.layers import Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D,  Dense, Flatten, Input\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Conv1DAutoEncoder(object):\n",
        "    model_name = 'con1d-auto-encoder'\n",
        "    VERBOSE = 1\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.time_window_size = None\n",
        "        self.metric = None\n",
        "        self.threshold = 5.0\n",
        "        self.config = None\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(time_window_size, metric):\n",
        "        # model = Sequential()\n",
        "        # model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu',\n",
        "        #                  input_shape=(time_window_size, 1)))\n",
        "        # model.add(GlobalMaxPool1D())\n",
        "        #\n",
        "        # model.add(Dense(units=time_window_size, activation='linear'))\n",
        "\n",
        "        input_layer = Input(shape=(time_window_size, 1))\n",
        "        conv1 = Conv1D(filters=256, kernel_size=5, padding='same', activation='relu')(input_layer)\n",
        "        pool1 = AveragePooling1D()(conv1)\n",
        "        conv2 = Conv1D(filters=125, kernel_size=5, padding='same', activation='relu')(pool1)\n",
        "\n",
        "        encoder = GlobalMaxPool1D()(conv2)\n",
        "        # encoder = GlobalAveragePooling1D()(conv1)\n",
        "        decoder = Dense(units=time_window_size, activation='linear')(encoder)\n",
        "        model_encoder = Model(inputs=input_layer, outputs=encoder)\n",
        "        # Input(shape=(encoding_dim,))\n",
        "        # model_decoder = Model(inputs=dim, outputs=decoder)\n",
        "        model = Model(inputs=input_layer, outputs=decoder)\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metric])\n",
        "        print(model.summary())\n",
        "        return model, model_encoder, decoder\n",
        "\n",
        "    @staticmethod\n",
        "    def get_config_file(model_dir_path):\n",
        "        return model_dir_path + '/' + Conv1DAutoEncoder.model_name + '-config.npy'\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weight_file(model_dir_path):\n",
        "        return model_dir_path + '/' + Conv1DAutoEncoder.model_name + '-weights.h5'\n",
        "\n",
        "    @staticmethod\n",
        "    def get_architecture_file(model_dir_path):\n",
        "        return model_dir_path + '/' + Conv1DAutoEncoder.model_name + '-architecture.json'\n",
        "\n",
        "    def load_model(self, model_dir_path):\n",
        "        config_file_path = self.get_config_file(model_dir_path)\n",
        "        self.config = np.load(config_file_path).item()\n",
        "        self.metric = self.config['metric']\n",
        "        self.time_window_size = self.config['time_window_size']\n",
        "        self.threshold = self.config['threshold']\n",
        "        self.model, self.encoder, self.decoder = self.create_model(self.time_window_size, self.metric)\n",
        "        # print('self.model: ', self.model)\n",
        "        weight_file_path = self.get_weight_file(model_dir_path)\n",
        "        self.model.load_weights(weight_file_path)\n",
        "\n",
        "    def fit(self, dataset, model_dir_path, batch_size=8, epochs=100, validation_split=0.1, metric='mean_absolute_error',\n",
        "            estimated_negative_sample_ratio=0.9):\n",
        "        self.time_window_size = dataset.shape[1]\n",
        "        self.metric = metric\n",
        "\n",
        "        input_timeseries_dataset = np.expand_dims(dataset, axis=2)\n",
        "\n",
        "        weight_file_path = self.get_weight_file(model_dir_path=model_dir_path)\n",
        "        architecture_file_path = self.get_architecture_file(model_dir_path)\n",
        "        checkpoint = ModelCheckpoint(weight_file_path)\n",
        "        self.model, self.encoder, self.decoder = self.create_model(self.time_window_size, metric=self.metric)\n",
        "        open(architecture_file_path, 'w').write(self.model.to_json())\n",
        "        history = self.model.fit(x=input_timeseries_dataset, y=dataset,\n",
        "                                 batch_size=batch_size, epochs=epochs,\n",
        "                                 verbose=self.VERBOSE, validation_split=validation_split,\n",
        "                                 callbacks=[checkpoint]).history\n",
        "        self.model.save_weights(weight_file_path)\n",
        "\n",
        "        scores = self.predict(dataset)\n",
        "        scores.sort()\n",
        "        cut_point = int(estimated_negative_sample_ratio * len(scores))\n",
        "        self.threshold = scores[cut_point]\n",
        "\n",
        "        print('estimated threshold is ' + str(self.threshold))\n",
        "\n",
        "        self.config = dict()\n",
        "        self.config['time_window_size'] = self.time_window_size\n",
        "        self.config['metric'] = self.metric\n",
        "        self.config['threshold'] = self.threshold\n",
        "        config_file_path = self.get_config_file(model_dir_path=model_dir_path)\n",
        "        np.save(config_file_path, self.config)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, timeseries_dataset):\n",
        "        input_timeseries_dataset = np.expand_dims(timeseries_dataset, axis=2)\n",
        "        target_timeseries_dataset = self.model.predict(x=input_timeseries_dataset)\n",
        "        dist = np.linalg.norm(timeseries_dataset - target_timeseries_dataset, axis=-1)\n",
        "        return dist\n",
        "\n",
        "    def anomaly(self, timeseries_dataset, threshold=None):\n",
        "        if threshold is not None:\n",
        "            self.threshold = threshold\n",
        "\n",
        "        dist = self.predict(timeseries_dataset)\n",
        "        return zip(dist >= self.threshold, dist)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TyrrF5IKKLb",
        "colab_type": "code",
        "outputId": "88d0e7e5-cbb5-4fc6-8125-fbace23efa13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "data_dir_path = './data'\n",
        "model_dir_path = './models'\n",
        "\n",
        "ae = Conv1DAutoEncoder()\n",
        "\n",
        "ae.fit(X_train, model_dir_path=model_dir_path, estimated_negative_sample_ratio=0.99, epochs=1)\n",
        "\n",
        "# load back the model saved in model_dir_path detect anomaly\n",
        "# ae.load_model(model_dir_path)\n",
        "# anomaly_information = ae.anomaly(ecg_np_data[:23, :])\n",
        "# reconstruction_error = []\n",
        "# for idx, (is_anomaly, dist) in enumerate(anomaly_information):\n",
        "#     print('# ' + str(idx) + ' is ' + ('abnormal' if is_anomaly else 'normal') + ' (dist: ' + str(dist) + ')')\n",
        "#     reconstruction_error.append(dist)\n",
        "\n",
        "# visualize_reconstruction_error(reconstruction_error, ae.threshold)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        (None, 12288, 1)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_34 (Conv1D)           (None, 12288, 256)        1536      \n",
            "_________________________________________________________________\n",
            "average_pooling1d_5 (Average (None, 6144, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_35 (Conv1D)           (None, 6144, 125)         160125    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_16 (Glo (None, 125)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 12288)             1548288   \n",
            "=================================================================\n",
            "Total params: 1,709,949\n",
            "Trainable params: 1,709,949\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 7546 samples, validate on 839 samples\n",
            "Epoch 1/1\n",
            "7546/7546 [==============================] - 1130s 150ms/step - loss: 0.0045 - mean_absolute_error: 0.0353 - val_loss: 8.5191e-04 - val_mean_absolute_error: 0.0198\n",
            "estimated threshold is 14.432010165173555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.004476935550980981],\n",
              " 'mean_absolute_error': [0.03528853397635734],\n",
              " 'val_loss': [0.0008519116360488813],\n",
              " 'val_mean_absolute_error': [0.019789328362406437]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFGlSxeCl_qU",
        "colab_type": "code",
        "outputId": "5be89c56-2910-44fc-a973-bb849bd693db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "ae.fit(X_train, model_dir_path=model_dir_path, estimated_negative_sample_ratio=0.99, epochs=4)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        (None, 12288, 1)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 12288, 256)        1536      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 12288)             3158016   \n",
            "=================================================================\n",
            "Total params: 3,159,552\n",
            "Trainable params: 3,159,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 7546 samples, validate on 839 samples\n",
            "Epoch 1/4\n",
            "7546/7546 [==============================] - 241s 32ms/step - loss: 0.8968 - mean_absolute_error: 0.5412 - val_loss: 0.3632 - val_mean_absolute_error: 0.3542\n",
            "Epoch 2/4\n",
            "7546/7546 [==============================] - 241s 32ms/step - loss: 0.8881 - mean_absolute_error: 0.5357 - val_loss: 0.3585 - val_mean_absolute_error: 0.3511\n",
            "Epoch 3/4\n",
            "7546/7546 [==============================] - 241s 32ms/step - loss: 0.8839 - mean_absolute_error: 0.5337 - val_loss: 0.3572 - val_mean_absolute_error: 0.3501\n",
            "Epoch 4/4\n",
            "7546/7546 [==============================] - 242s 32ms/step - loss: 0.8827 - mean_absolute_error: 0.5333 - val_loss: 0.3562 - val_mean_absolute_error: 0.3493\n",
            "estimated threshold is 322.37317836090784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.8967701248960851,\n",
              "  0.8880672248200336,\n",
              "  0.8838524784613376,\n",
              "  0.8826933048572713],\n",
              " 'mean_absolute_error': [0.5412222944849718,\n",
              "  0.5356636170743159,\n",
              "  0.5336996646607362,\n",
              "  0.5332512512167221],\n",
              " 'val_loss': [0.36324655575426884,\n",
              "  0.35849148620261906,\n",
              "  0.35721674406095116,\n",
              "  0.3561796826111759],\n",
              " 'val_mean_absolute_error': [0.35421823487307375,\n",
              "  0.351147339342959,\n",
              "  0.35012875504445407,\n",
              "  0.34928929481624277]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWcWwIDRVpXF",
        "colab_type": "code",
        "outputId": "6aedd449-9f6d-4da0-bdfb-66e60bce35c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "print(ae.encoder.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 12288, 1)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 12288, 256)        1536      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_6 (Glob (None, 256)               0         \n",
            "=================================================================\n",
            "Total params: 1,536\n",
            "Trainable params: 1,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW9Ks3volQT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_timeseries_dataset = ae.anomaly(X_test)\n",
        "# dist = np.linalg.norm(X_test - target_timeseries_dataset, axis=-1)\n",
        "\n",
        "Ypred = []\n",
        "reconstruction_error = []\n",
        "for idx, (is_anomaly, dist) in enumerate(target_timeseries_dataset):\n",
        "      predicted_label = 1 if is_anomaly else 0\n",
        "      Ypred.append(predicted_label)\n",
        "      reconstruction_error.append(dist)\n",
        "\n",
        "error_sum = []\n",
        "labels = []\n",
        "prev = 0\n",
        "\n",
        "for i in range(len(reconstruction_error) // 5):\n",
        "    sub_distance = reconstruction_error[prev:prev + 5]\n",
        "    sub_labels = Ypred[prev:prev + 5]\n",
        "    prev = prev + 5\n",
        "    error_sum.append(np.sum(sub_distance))\n",
        "    labels.append(np.max(sub_labels))\n",
        "\n",
        "\n",
        "# visualize_reconstruction_error(reconstruction_error, ae.threshold)\n",
        "np.savetxt(r\"distances.csv\", np.c_[error_sum,labels], comments='',\n",
        "           header=\"error,label\",\n",
        "           fmt='%f', delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkO-wuc4U4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_predicted = ae.anomaly(X_train)\n",
        "test_predicted = ae.anomaly(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x8PGV-tAVTk",
        "colab_type": "code",
        "outputId": "9bb7bd8e-7084-4fa5-bcf5-ba8bb3b71ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train = list(train_predicted)\n",
        "y_test = list(test_predicted)\n",
        "dist_train = [x[1] for x in y_train]\n",
        "dist_test = [x[1] for x in y_test]\n",
        "# print(\"dist_train: \", train_predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dist_train:  <zip object at 0x7f6114a10a08>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoZO5r_49Dam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = OneClassSVM(nu=0.25, gamma=\"auto\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS83VgY-93HH",
        "colab_type": "code",
        "outputId": "e8f7a6d5-81b3-4ed3-e5d5-dd23e9e38726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# print(train_predicted.shape)\n",
        "clf.fit(np.array(dist_train).reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n",
              "            max_iter=-1, nu=0.25, random_state=None, shrinking=True, tol=0.001,\n",
              "            verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2E8o4K39tBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_test = clf.predict(np.array(dist_test).reshape(-1,1))\n",
        "y_pred_test[y_pred_test == 1] = 0\n",
        "y_pred_test[y_pred_test == -1] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voYTXCjfB89u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_sum = []\n",
        "labels = []\n",
        "prev = 0\n",
        "\n",
        "for i in range(len(y_pred_test) // 5):\n",
        "    sub_labels = y_pred_test[prev:prev + 5]\n",
        "    prev = prev + 5\n",
        "    # error_sum.append(np.sum(sub_distance))\n",
        "    labels.append(np.max(sub_labels))\n",
        "\n",
        "\n",
        "np.savetxt(r\"svd_res.csv\", np.c_[labels], comments='',\n",
        "           header=\"anomaly\",\n",
        "           fmt='%f', delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3nD-UPvUY_R",
        "colab_type": "code",
        "outputId": "2fe9982f-be8e-47b8-8cc9-23af277ce6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#new Features\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "X_train = X_train.reshape(-1,X_train.shape[1],1)\n",
        "X_test = X_test.reshape(-1,X_test.shape[1],1)\n",
        "print(X_train.shape)\n",
        "X_train_encoded = ae.encoder.predict(X_train)\n",
        "X_test_encoded = ae.encoder.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8385, 12288, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj3br9zmXWVe",
        "colab_type": "code",
        "outputId": "f77d7721-eb77-41f3-e010-b1ceda329e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "clf = IsolationForest(n_estimators=100,   behaviour='new',\n",
        "                      contamination='auto', verbose=1, n_jobs=6)\n",
        "# clf =EllipticEnvelope()\n",
        "# clf = OneClassSVM(nu=0.3, gamma=\"auto\")\n",
        "clf.fit(X_train_encoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=6)]: Done   2 out of   6 | elapsed:    0.4s remaining:    0.8s\n",
            "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsolationForest(behaviour='new', bootstrap=False, contamination='auto',\n",
              "                max_features=1.0, max_samples='auto', n_estimators=100,\n",
              "                n_jobs=6, random_state=None, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebUIcen2YMHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_test = clf.predict(X_test_encoded)\n",
        "y_pred_test[y_pred_test == 1] = 0\n",
        "y_pred_test[y_pred_test == -1] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lEpranTYWQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_sum = []\n",
        "labels = []\n",
        "prev = 0\n",
        "\n",
        "for i in range(len(y_pred_test) // 5):\n",
        "    sub_labels = y_pred_test[prev:prev + 5]\n",
        "    prev = prev + 5\n",
        "    # error_sum.append(np.sum(sub_distance))\n",
        "    labels.append(np.max(sub_labels))\n",
        "\n",
        "\n",
        "np.savetxt(r\"encoded_IF_res.csv\", np.c_[labels], comments='',\n",
        "           header=\"anomaly\",\n",
        "           fmt='%f', delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bD1sScMGYuL",
        "colab_type": "code",
        "outputId": "18642053-8678-472a-d642-53e0eca88038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "truth =  pd.read_csv(\"/content/drive/My Drive/MLDM Project/labels.csv\", sep=\",\")\n",
        "truth = truth[\"anomaly\"].to_numpy().astype(int)\n",
        "truth = truth.reshape(-1, 1)\n",
        "\n",
        "print(\"results: \", classification_report(labels, truth))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results:                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93       327\n",
            "           1       0.88      0.98      0.93       267\n",
            "\n",
            "    accuracy                           0.93       594\n",
            "   macro avg       0.93      0.93      0.93       594\n",
            "weighted avg       0.93      0.93      0.93       594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}